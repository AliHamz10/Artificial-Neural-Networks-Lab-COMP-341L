{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Perceptron vs Adaline and the XOR Problem\n",
    "\n",
    "Foundational Models \u2013 Perceptron vs. Adaline and the XOR Problem.  \n",
    "All implementations use **NumPy only**; data in **bipolar format** (-1 and +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset creation (bipolar format)\n",
    "\n",
    "AND, OR, and XOR truth tables with inputs and targets in **bipolar** form: -1 and +1 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AND:\n",
      "X = [[ 1.  1.]\n",
      " [ 1. -1.]\n",
      " [-1.  1.]\n",
      " [-1. -1.]]\n",
      "y = [ 1. -1. -1. -1.]\n",
      "\n",
      "OR:\n",
      "X = [[ 1.  1.]\n",
      " [ 1. -1.]\n",
      " [-1.  1.]\n",
      " [-1. -1.]]\n",
      "y = [ 1.  1.  1. -1.]\n",
      "\n",
      "XOR:\n",
      "X = [[ 1.  1.]\n",
      " [ 1. -1.]\n",
      " [-1.  1.]\n",
      " [-1. -1.]]\n",
      "y = [-1.  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bipolar logic: -1 and +1 only (no 0/1)\n",
    "# AND: (1,1)->1; (1,-1)->-1; (-1,1)->-1; (-1,-1)->-1\n",
    "X_and = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]], dtype=np.float64)\n",
    "y_and = np.array([1, -1, -1, -1], dtype=np.float64)\n",
    "\n",
    "# OR: (1,1)->1; (1,-1)->1; (-1,1)->1; (-1,-1)->-1\n",
    "X_or = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]], dtype=np.float64)\n",
    "y_or = np.array([1, 1, 1, -1], dtype=np.float64)\n",
    "\n",
    "# XOR: (1,1)->-1; (1,-1)->1; (-1,1)->1; (-1,-1)->-1\n",
    "X_xor = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]], dtype=np.float64)\n",
    "y_xor = np.array([-1, 1, 1, -1], dtype=np.float64)\n",
    "\n",
    "print(\"AND:\")\n",
    "print(\"X =\", X_and)\n",
    "print(\"y =\", y_and)\n",
    "print(\"\\nOR:\")\n",
    "print(\"X =\", X_or)\n",
    "print(\"y =\", y_or)\n",
    "print(\"\\nXOR:\")\n",
    "print(\"X =\", X_xor)\n",
    "print(\"y =\", y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: EDA \u2013 Linear (in)separability plots\n",
    "\n",
    "2D scatter: x1 vs x2, coloured by target (+1 vs -1). AND and OR are linearly separable; XOR is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X, y, title, ax):\n",
    "    \"\"\"Scatter x1 vs x2, colour/marker by target (+1 or -1).\"\"\"\n",
    "    pos = y == 1\n",
    "    neg = y == -1\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], c='C0', marker='o', s=80, label='+1', edgecolors='k')\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], c='C1', marker='s', s=80, label='-1', edgecolors='k')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "plot_dataset(X_and, y_and, 'AND \u2013 linearly separable', axes[0])\n",
    "plot_dataset(X_or, y_or, 'OR \u2013 linearly separable', axes[1])\n",
    "plot_dataset(X_xor, y_xor, 'XOR \u2013 not linearly separable', axes[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perceptron (from scratch)\n",
    "\n",
    "- **Update rule:** \u0394w_i = \u03b1 * x_i * t only when predicted output \u2260 target t.\n",
    "- **Activation:** binary step: output = +1 if net \u2265 0, else -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Perceptron with bipolar step activation. Update only on misclassification: \u0394w_i = \u03b1 * x_i * t.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features, learning_rate=0.1, max_epochs=100):\n",
    "        # weights include bias (first weight); input will be augmented with 1\n",
    "        self.w = np.zeros(n_features + 1)\n",
    "        self.alpha = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    @staticmethod\n",
    "    def step(net):\n",
    "        \"\"\"Binary step: +1 if net >= 0, else -1.\"\"\"\n",
    "        return np.where(net >= 0, 1.0, -1.0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_aug = np.column_stack([np.ones(len(X)), X])  # bias term\n",
    "        history = []\n",
    "        for epoch in range(self.max_epochs):\n",
    "            misclass = 0\n",
    "            for i in range(len(X_aug)):\n",
    "                net = np.dot(self.w, X_aug[i])\n",
    "                out = self.step(net)\n",
    "                if out != y[i]:\n",
    "                    # Perceptron rule: \u0394w = \u03b1 * x * t (target t, not error)\n",
    "                    self.w += self.alpha * y[i] * X_aug[i]\n",
    "                    misclass += 1\n",
    "            history.append(misclass)\n",
    "            if misclass == 0:\n",
    "                break\n",
    "        return np.array(history)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_aug = np.column_stack([np.ones(len(X)), X])\n",
    "        net = X_aug @ self.w\n",
    "        return self.step(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Perceptron AND: converged in 3 epochs\n",
      "Perceptron OR: converged in 3 epochs\n",
      "AND predictions: [ 1. -1. -1. -1.]\n",
      "OR predictions: [ 1.  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Train Perceptron on AND and OR\n",
    "perc_and = Perceptron(n_features=2, learning_rate=0.1, max_epochs=100)\n",
    "hist_and = perc_and.fit(X_and, y_and)\n",
    "\n",
    "perc_or = Perceptron(n_features=2, learning_rate=0.1, max_epochs=100)\n",
    "hist_or = perc_or.fit(X_or, y_or)\n",
    "\n",
    "print('Perceptron AND: converged in', len(hist_and), 'epochs')\n",
    "print('Perceptron OR: converged in', len(hist_or), 'epochs')\n",
    "print('AND predictions:', perc_and.predict(X_and))\n",
    "print('OR predictions:', perc_or.predict(X_or))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(ax, w, xlim=(-1.5, 1.5), ylim=(-1.5, 1.5)):\n",
    "    \"\"\"Plot line w0 + w1*x1 + w2*x2 = 0 => x2 = -(w0 + w1*x1)/w2 (if w2 != 0).\"\"\"\n",
    "    w0, w1, w2 = w[0], w[1], w[2]\n",
    "    if np.abs(w2) < 1e-9:\n",
    "        return\n",
    "    x1 = np.linspace(xlim[0], xlim[1], 100)\n",
    "    x2 = -(w0 + w1 * x1) / w2\n",
    "    ax.plot(x1, x2, 'k--', linewidth=2, label='Decision boundary')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot_dataset(X_and, y_and, 'AND \u2013 Perceptron decision boundary', axes[0])\n",
    "plot_decision_boundary(axes[0], perc_and.w)\n",
    "axes[0].legend()\n",
    "\n",
    "plot_dataset(X_or, y_or, 'OR \u2013 Perceptron decision boundary', axes[1])\n",
    "plot_decision_boundary(axes[1], perc_or.w)\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Adaline (from scratch)\n",
    "\n",
    "- **Update rule:** \u0394w_i = \u03b1 * (t - y_in) * x_i, where y_in = w^T x (net input, no step in update).\n",
    "- **Training:** linear activation (output = y_in); MSE computed on y_in vs t.\n",
    "- **Classification:** threshold: class +1 if y_in \u2265 0, else -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline:\n",
    "    \"\"\"Adaline (Widrow-Hoff / delta rule). Training uses linear output (y_in); classification uses threshold.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features, learning_rate=0.1, max_epochs=100, random_state=None):\n",
    "        # random_state=None => zero init (for AND/OR). Set an int so XOR gets small random init and a non-degenerate boundary.\n",
    "        if random_state is None:\n",
    "            self.w = np.zeros(n_features + 1)\n",
    "        else:\n",
    "            self.w = np.random.RandomState(random_state).randn(n_features + 1) * 0.01\n",
    "        self.alpha = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_aug = np.column_stack([np.ones(len(X)), X])\n",
    "        mse_history = []\n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Sequential (online) updates: \u0394w_i = \u03b1 * (t - y_in) * x_i per sample\n",
    "            for i in range(len(X_aug)):\n",
    "                y_in = np.dot(self.w, X_aug[i])\n",
    "                self.w += self.alpha * (y[i] - y_in) * X_aug[i]\n",
    "            y_in_all = X_aug @ self.w\n",
    "            mse = np.mean((y - y_in_all) ** 2)\n",
    "            mse_history.append(mse)\n",
    "            if mse < 1e-6:\n",
    "                break\n",
    "        return np.array(mse_history)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_aug = np.column_stack([np.ones(len(X)), X])\n",
    "        y_in = X_aug @ self.w\n",
    "        return np.where(y_in >= 0, 1.0, -1.0)  # threshold for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adaline AND: final MSE = 0.25432525951557095 , epochs = 500\n",
      "Adaline OR: final MSE = 0.25432525951557095 , epochs = 500\n",
      "AND predictions: [ 1. -1. -1. -1.]\n",
      "OR predictions: [ 1.  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Train Adaline on AND and OR; track MSE (zero init so they converge)\n",
    "ada_and = Adaline(n_features=2, learning_rate=0.1, max_epochs=500)\n",
    "mse_and = ada_and.fit(X_and, y_and)\n",
    "\n",
    "ada_or = Adaline(n_features=2, learning_rate=0.1, max_epochs=500)\n",
    "mse_or = ada_or.fit(X_or, y_or)\n",
    "\n",
    "print('Adaline AND: final MSE =', mse_and[-1], ', epochs =', len(mse_and))\n",
    "print('Adaline OR: final MSE =', mse_or[-1], ', epochs =', len(mse_or))\n",
    "print('AND predictions:', ada_and.predict(X_and))\n",
    "print('OR predictions:', ada_or.predict(X_or))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(mse_and, 'C0-')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('Adaline on AND \u2013 MSE vs epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(mse_or, 'C1-')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('Adaline on OR \u2013 MSE vs epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: XOR \u2013 demonstrating the limitation\n",
    "\n",
    "Train Perceptron and Adaline on XOR. Neither can separate the data; document behaviour and (for report) inequality contradictions and decision boundary equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Perceptron on XOR: misclassifications per epoch (last 10): [4 4 4 4 4 4 4 4 4 4]\n",
      "Never converges: misclassifications remain > 0.\n"
     ]
    }
   ],
   "source": [
    "# Perceptron on XOR \u2013 does not converge\n",
    "perc_xor = Perceptron(n_features=2, learning_rate=0.1, max_epochs=500)\n",
    "hist_xor = perc_xor.fit(X_xor, y_xor)\n",
    "\n",
    "print('Perceptron on XOR: misclassifications per epoch (last 10):', hist_xor[-10:])\n",
    "print('Never converges: misclassifications remain > 0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(hist_xor)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Misclassifications')\n",
    "plt.title('Perceptron on XOR \u2013 does not converge')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adaline on XOR: final weights (bias, w1, w2) = [-1.38777878e-17  5.88235294e-02  1.17647059e-01]\n",
      "Decision boundary: -0.0000 + 0.0588*x1 + 0.1176*x2 = 0\n",
      "Final MSE = 1.0173010380622838\n"
     ]
    }
   ],
   "source": [
    "# Adaline on XOR \u2013 MSE does not go to zero; single line cannot separate\n",
    "ada_xor = Adaline(n_features=2, learning_rate=0.1, max_epochs=500, random_state=44)\n",
    "mse_xor = ada_xor.fit(X_xor, y_xor)\n",
    "\n",
    "print('Adaline on XOR: final weights (bias, w1, w2) =', ada_xor.w)\n",
    "print('Decision boundary: {:.4f} + {:.4f}*x1 + {:.4f}*x2 = 0'.format(ada_xor.w[0], ada_xor.w[1], ada_xor.w[2]))\n",
    "print('Final MSE =', mse_xor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(mse_xor)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Adaline on XOR \u2013 MSE does not converge to zero')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot Adaline decision boundary on XOR (cannot separate the two classes)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_dataset(X_xor, y_xor, 'XOR \u2013 Adaline decision boundary (cannot separate)', ax)\n",
    "plot_decision_boundary(ax, ada_xor.w)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Multi-layer network for XOR (conceptual + forward pass)\n",
    "\n",
    "2\u20132\u20131 architecture: 2 inputs + bias, 2 hidden (step), 1 output (step). Weights set manually so the network computes XOR (no learning). See report for diagram and weight justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi-layer XOR predictions: [-1.  1.  1. -1.]\n",
      "Targets:               [-1.  1.  1. -1.]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "def step(z):\n",
    "    return np.where(z >= 0, 1.0, -1.0)\n",
    "\n",
    "# Weights set by hand: H1 = AND (fires only at (1,1)); H2 = OR (fires for (1,1),(1,-1),(-1,1)).\n",
    "# Output = step(-1 - H1 + H2) => +1 only when H1=-1 and H2=+1 (XOR). See report for full derivation.\n",
    "\n",
    "W_h = np.array([\n",
    "    [-1.5, 1, 1],   # H1: AND\n",
    "    [0.5, 1, 1]     # H2: OR\n",
    "])\n",
    "W_out = np.array([-1.0, -1.0, 1.0])  # bias, H1, H2 => output = step(-1 - H1 + H2)\n",
    "\n",
    "def xor_forward(X):\n",
    "    \"\"\"Forward pass for 2-2-1 XOR network with step activation.\"\"\"\n",
    "    ones = np.ones((len(X), 1))\n",
    "    X_aug = np.column_stack([ones, X])\n",
    "    H_in = X_aug @ W_h.T   # (n, 2)\n",
    "    H = step(H_in)\n",
    "    H_aug = np.column_stack([ones, H])\n",
    "    out_in = H_aug @ W_out\n",
    "    return step(out_in)\n",
    "\n",
    "xor_pred = xor_forward(X_xor)\n",
    "print('Multi-layer XOR predictions:', xor_pred)\n",
    "print('Targets:              ', y_xor)\n",
    "print('Match:', np.allclose(xor_pred, y_xor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}